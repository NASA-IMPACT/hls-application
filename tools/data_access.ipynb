{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a3c0c3-4316-42bb-83ed-bc224e6c0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import odc.stac\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.env import Env\n",
    "import os\n",
    "\n",
    "def get_HLS_data(lat_range,lon_range,baseline_year,analysis_year):    \n",
    "    temporal = (str(baseline_year)+\"-01-01\",str(analysis_year)+\"-12-31\")\n",
    "    \n",
    "    results = earthaccess.search_data(\n",
    "    short_name=['HLSL30','HLSS30'],\n",
    "    bounding_box=(min(lon_range),min(lat_range),max(lon_range),max(lat_range)),\n",
    "    temporal=temporal,\n",
    "    )\n",
    "    \n",
    "    df = pd.json_normalize(results)\n",
    "\n",
    "    return df, results\n",
    "    \n",
    "def filter_items_by_id(item, keyword):\n",
    "    return keyword in item.id\n",
    "\n",
    "    \n",
    "def search_cmr_stac(baseline_year,analysis_year,lat_range,lon_range,date_begin=None,date_end=None,keyword=None):\n",
    "    bbox = ([min(lon_range),min(lat_range),max(lon_range),max(lat_range)])\n",
    "    years = np.arange(baseline_year,analysis_year+1,1)\n",
    "    \n",
    "    # Due to the limitation of the search, split into every year to perform the search\n",
    "    # Note that this might work for a city scale, but for larger domain, the search can\n",
    "    # even further needes to be constrained \n",
    "    items_list = list()\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        collections=['HLSL30.v2.0', 'HLSS30.v2.0']\n",
    "        url='https://cmr.earthdata.nasa.gov/cloudstac/LPCLOUD'\n",
    "        cloudcover_max=50\n",
    "        lim=100\n",
    "        if date_begin is not None:\n",
    "            dt_min = str(year)+'-'+date_begin\n",
    "            dt_max = str(year)+'-'+date_end\n",
    "        else:\n",
    "            dt_min = str(year)+'-01-01'\n",
    "            dt_max = str(year)+'-12-31'\n",
    "            \n",
    "        # open the catalog\n",
    "        catalog = pystac_client.Client.open(f'{url}')\n",
    "        \n",
    "        # perform the search\n",
    "        search = catalog.search(\n",
    "            collections=collections,\n",
    "            bbox=bbox,\n",
    "            datetime=dt_min + '/' + dt_max,\n",
    "            limit=lim\n",
    "        )\n",
    "    \n",
    "        items = list(search.items())\n",
    "\n",
    "        if keyword is not None:\n",
    "            items = [item for item in items if keyword in item.id]\n",
    "                    \n",
    "        \n",
    "        items_list.append(items)\n",
    "        print('Found', len(items), 'granules at point', bbox, 'from', dt_min, 'to', dt_max)\n",
    "\n",
    "    \n",
    "    items_list = [i for item in items_list for i in item]\n",
    "\n",
    "    return items_list\n",
    "\n",
    "\n",
    "def rename_common_bands(rename_band,S30_band,L30_band,items_list):\n",
    "    # Rename HLSS B8A and HLSL B05 to common band name\n",
    "    for item in items_list:\n",
    "        if \"HLS.L30\" in item.id:\n",
    "            item.assets[rename_band] = item.assets.pop(L30_band)\n",
    "        if \"HLS.S30\" in item.id:\n",
    "            item.assets[rename_band] = item.assets.pop(S30_band)\n",
    "    return items_list\n",
    "\n",
    "def load_odc_stac(crs,bands,spatial_res,items_list,bbox):\n",
    "\n",
    "    # Set CRS and resolution, open lazily with odc.stac\n",
    "    ds = odc.stac.stac_load(\n",
    "        items_list,\n",
    "        bbox=bbox,\n",
    "        bands=(bands),\n",
    "        crs=crs,\n",
    "        resolution=spatial_res,\n",
    "        #chunks=None,\n",
    "        chunks={\"band\":1,\"x\":512,\"y\":512},  # If empty, chunks along band dim, \n",
    "        #groupby=\"solar_day\", # This limits to first obs per day\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "    \n",
    "\n",
    "def load_dask(ds):\n",
    "\n",
    "    import rasterio\n",
    "    gdal_cookiefile = '~/cookies.txt'\n",
    "    gdal_cookiejar = '~/cookies.txt'\n",
    "    gdal_disable_read = 'EMPTY_DIR'\n",
    "    gdal_curl_extensions = 'TIF'\n",
    "    gdal_unsafessl = 'YES'\n",
    "    \n",
    "    gdal_config = {\n",
    "    \"GDAL_HTTP_COOKIEFILE\": '~/cookies.txt',\n",
    "    \"GDAL_HTTP_COOKIEJAR\": '~/cookies.txt',\n",
    "    \"GDAL_DISABLE_READDIR_ON_OPEN\": 'YES',\n",
    "    \"CPL_VSIL_CURL_ALLOWED_EXTENSIONS\": 'TIF',\n",
    "    \"GDAL_HTTP_UNSAFESSL\": 'YES',\n",
    "    \"CPL_VSIL_CURL_USE_HEAD\":False,\n",
    "    #\"CPL_CURL_VERBOSE\":True\n",
    "    }\n",
    "    max_retries = 1\n",
    "    retry_delay = 5 # seconds to wait between retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with rasterio.Env(**gdal_config): # Setting up GDAL environment\n",
    "                ds.load()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return ds\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def load_hls_url(link,chunk_size):\n",
    "    # create an empty dataset xarray\n",
    "    gdal_config = {\n",
    "    \"GDAL_HTTP_COOKIEFILE\": '~/cookies.txt',\n",
    "    \"GDAL_HTTP_COOKIEJAR\": '~/cookies.txt',\n",
    "    \"GDAL_DISABLE_READDIR_ON_OPEN\": 'YES',\n",
    "    \"CPL_VSIL_CURL_ALLOWED_EXTENSIONS\": 'TIF',\n",
    "    \"GDAL_HTTP_UNSAFESSL\": 'YES',\n",
    "    \"CPL_VSIL_CURL_USE_HEAD\":False,\n",
    "    #\"CPL_CURL_VERBOSE\":True\n",
    "    }\n",
    "    \n",
    "    rasterio.Env(**gdal_config)\n",
    "    \n",
    "    band = link.split('/')[-1].split('.')[-2]\n",
    "    array = rxr.open_rasterio(link, chunks=chunk_size, masked=True, name = band).squeeze('band', drop=True).load()\n",
    "    array.name = band\n",
    "    if 'Fmask' not in band:\n",
    "        array.attrs['scale_factor'] = 0.0001 \n",
    "    else:\n",
    "        array.attrs['scale_factor'] = 1\n",
    "    \n",
    "    return array\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b34218-3c06-4405-a973-2fd8ee827aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geometry_clip(city_name):\n",
    "    from shapely.geometry import box\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    df_geo = gpd.read_file('cb_2018_us_ua10_500k/cb_2018_us_ua10_500k.shp')\n",
    "    df_geo_loc = df_geo.loc[df_geo['NAME10'] == city_name]\n",
    "    \n",
    "    # create a bounding box from the shapefile \n",
    "    bounds = df_geo_loc.geometry.bounds.values[0]\n",
    "    geom = box(*bounds)\n",
    "    \n",
    "    df_geo_out = gpd.GeoDataFrame({\"id\":1,\"geometry\":[box(*bounds)]})\n",
    "    df_geo_out = df_geo_out.set_geometry('geometry')\n",
    "    df_geo_out.crs = df_geo.crs\n",
    "\n",
    "    return df_geo_out\n",
    "\n",
    "def scale_hls_data(ds,bands):\n",
    "\n",
    "    for band in bands:\n",
    "\n",
    "        if 'Fmask' not in band:\n",
    "    \n",
    "            ds[band].data = 0.0001 * ds[band].data\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af807d52-1f79-479b-a1d1-48e9868b962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def configure_gdal_rasterio_dask():\n",
    "    # ----------------- Step 1: 1. Monkey‑patch Xarray’s .load() to wrap every read in your Env ----------------- \n",
    "    # 1. Grab the real load method *before* patching\n",
    "    _orig_ds_load = xr.Dataset.load\n",
    "    _orig_da_load = xr.DataArray.load\n",
    "    _orig_da_compute = xr.Dataset.compute\n",
    "    \n",
    "    def _load_with_env(self, **kwargs):\n",
    "        # 2. In your Env you can set any GDAL/Rasterio opts\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            # 3. Call the *original* load, not xr.Dataset.load\n",
    "            return _orig_ds_load(self, **kwargs)\n",
    "    \n",
    "    def _da_load_with_env(self, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            return _orig_da_load(self, **kwargs)\n",
    "            \n",
    "    def _da_compute_with_env(self, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            return _orig_da_compute(self, **kwargs)\n",
    "    \n",
    "    # 4. Now monkey‑patch\n",
    "    xr.Dataset.load   = _load_with_env\n",
    "    xr.DataArray.load = _da_load_with_env\n",
    "    xr.Dataset.compute = _da_compute_with_env\n",
    "    # ----------------- Step 2: Monkey‑patch rasterio.open itself ----------------- \n",
    "    \n",
    "\n",
    "    # 1. Capture the true open\n",
    "    _orig_open = rasterio.open\n",
    "    \n",
    "    def open_with_env(*args, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            # 2. Call real open\n",
    "            return _orig_open(*args, **kwargs)\n",
    "    \n",
    "    # 3. Replace it\n",
    "    rasterio.open = open_with_env\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48753f7-3e8f-47d5-b88b-90c289b6811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "     # ----------------- Step 3: Ensure every Dask worker has the GDAL env set before they read ----------------- \n",
    "    \n",
    "def _setup_gdal():\n",
    "    \n",
    "    os.environ.update({\n",
    "        \"GDAL_DISABLE_READDIR_ON_OPEN\" : \"EMPTY_DIR\",\n",
    "        \"CPL_VSIL_CURL_ALLOWED_EXTENSIONS\" : \"TIF\",\n",
    "        \"GDAL_HTTP_COOKIEFILE\" : '~/cookies.txt',\n",
    "        \"GDAL_HTTP_COOKIEJAR\" : '~/cookies.txt',\n",
    "        \"GDAL_HTTP_UNSAFESSL\" : 'YES',\n",
    "        \"GDAL_HTTP_MAX_RETRY\" : '10',\n",
    "        \"GDAL_HTTP_RETRY_DELAY\" : '0.5',\n",
    "        \"CPL_VSIL_CURL_USE_HEAD\" : 'YES'\n",
    "        })\n",
    "    from rasterio.env import Env\n",
    "    Env().__enter__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283f6822-a40c-40b4-b157-805dd104eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pystac_to_json(item_list):\n",
    "    import pandas as pd\n",
    "    from shapely.geometry import Polygon\n",
    "    data = [i.geometry for i in item_list]\n",
    "    parsed_data = [[item['type'],Polygon(item['coordinates'][0])] for item in data]\n",
    "    df_geo = gpd.GeoDataFrame(data=parsed_data, columns=['type','geometry'])\n",
    "\n",
    "    data_id = [i.id for i in item_list]\n",
    "    df_geo['id'] = data_id\n",
    "    df_geo['granule_id'] = [i.split('.')[2] for i in df_geo['id']]\n",
    "    df_geo['sat_id'] = [i.split('.')[1] for i in df_geo['id']]\n",
    "    df_geo['date'] = [i.split('.')[3].split('T')[0] for i in df_geo['id']]\n",
    "    \n",
    "    return df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f01fa8c-f25f-40ac-b681-e3642073b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "from typing import List, Optional, Union\n",
    "import pystac\n",
    "\n",
    "def fetch_all_items_parallel(\n",
    "    stac_url: str,\n",
    "    collections: Optional[List[str]] = None,\n",
    "    datetime: Optional[str] = None,\n",
    "    bbox: Optional[List[float]] = None,\n",
    "    query: Optional[dict] = None,\n",
    "    max_workers: int = 10,\n",
    "    page_limit: int = 100,\n",
    "    return_items: bool = False\n",
    ") -> List[Union[dict, pystac.Item]]:\n",
    "    \"\"\"\n",
    "    Fetch all STAC items using parallel requests across paginated responses.\n",
    "    \n",
    "    Args:\n",
    "        stac_url (str): The URL of the STAC API.\n",
    "        collections (list): Optional list of collection IDs.\n",
    "        datetime (str): Optional ISO8601 datetime range.\n",
    "        bbox (list): Optional bounding box [xmin, ymin, xmax, ymax].\n",
    "        query (dict): Optional query parameters.\n",
    "        max_workers (int): Number of parallel threads.\n",
    "        page_limit (int): Number of items per page (if supported).\n",
    "        return_items (bool): If True, return as pystac.Items; else return dicts.\n",
    "        \n",
    "    Returns:\n",
    "        List of STAC Items (either dicts or pystac.Item objects).\n",
    "    \"\"\"\n",
    "    client = Client.open(stac_url)\n",
    "    \n",
    "    # Initial paged search\n",
    "    search = client.search(\n",
    "        collections=collections,\n",
    "        datetime=datetime,\n",
    "        bbox=bbox,\n",
    "        query=query,\n",
    "        limit=page_limit\n",
    "    )\n",
    "\n",
    "    # First page\n",
    "    all_items = list(search.get_items())\n",
    "    next_link = search._paging_info.get(\"next\")\n",
    "\n",
    "    # Helper: fetch one page\n",
    "    def fetch_page(url):\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return data.get(\"features\", [])\n",
    "\n",
    "    # Gather all remaining \"next\" URLs\n",
    "    next_urls = []\n",
    "    current = next_link\n",
    "    while current:\n",
    "        next_urls.append(current)\n",
    "        r = requests.get(current)\n",
    "        r.raise_for_status()\n",
    "        links = r.json().get(\"links\", [])\n",
    "        current = next((link[\"href\"] for link in links if link.get(\"rel\") == \"next\"), None)\n",
    "\n",
    "    # Parallel fetch\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(fetch_page, url) for url in next_urls]\n",
    "        for future in as_completed(futures):\n",
    "            all_items.extend(future.result())\n",
    "\n",
    "    # Convert to pystac.Items if needed\n",
    "    if return_items:\n",
    "        return [pystac.Item.from_dict(item) for item in all_items]\n",
    "    return all_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666ed2b-b0ff-49bc-868e-306bce3cc582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
