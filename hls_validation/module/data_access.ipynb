{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3c0c3-4316-42bb-83ed-bc224e6c0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import odc.stac\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.env import Env\n",
    "import os\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from rasterio.transform import xy\n",
    "from pyproj import Transformer\n",
    "import rioxarray as rxr\n",
    "\n",
    "def list_all_s3_files_anonymous(bucket_name, prefix=None):\n",
    "    \"\"\"\n",
    "    List all files from a public S3 bucket anonymously (no credentials needed).\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the public S3 bucket.\n",
    "        prefix (str, optional): Filter by a folder or prefix path.\n",
    "\n",
    "    Returns:\n",
    "        list: List of S3 object keys (filenames).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    kwargs = {\"Bucket\": bucket_name}\n",
    "    if prefix:\n",
    "        kwargs[\"Prefix\"] = prefix\n",
    "\n",
    "    files = []\n",
    "    for page in paginator.paginate(**kwargs):\n",
    "        files.extend([obj[\"Key\"] for obj in page.get(\"Contents\", [])])\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "def raster_center_latlon(path):\n",
    "    da = rxr.open_rasterio(path)\n",
    "    h, w = da.rio.height, da.rio.width\n",
    "    x, y = da.rio.transform() * (w/2, h/2)\n",
    "    return Transformer.from_crs(da.rio.crs, \"EPSG:4326\", always_xy=True).transform(x, y)\n",
    "\n",
    "\n",
    "\n",
    "def search_cmr_stac(start_year,end_year,lat_range,lon_range,date_begin=None,date_end=None,mgrs_tile_id=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Searches the CMR STAC API for HLS granules within a specified spatial and temporal range.\n",
    "\n",
    "    This function queries the HLSL30.v2.0 and HLSS30.v2.0 collections from NASA's LPCLOUD STAC endpoint.\n",
    "    The search is split by year to handle API limitations and is most suitable for small to medium spatial domains.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_year : int or str\n",
    "        The beginning year of the search period. Can be an integer (e.g., 2020) or a string (e.g., '2020-01-01').\n",
    "    end_year : int or str\n",
    "        The ending year of the search period (inclusive). Same format as start_year.\n",
    "    lat_range : list or tuple\n",
    "        A pair of latitude values specifying the search bounds, e.g., [min_lat, max_lat].\n",
    "    lon_range : list or tuple\n",
    "        A pair of longitude values specifying the search bounds, e.g., [min_lon, max_lon].\n",
    "    date_begin : str, optional\n",
    "        Optional start date (MM-DD) for each year, e.g., '06-01'. If None, defaults to '01-01'.\n",
    "    date_end : str, optional\n",
    "        Optional end date (MM-DD) for each year, e.g., '09-30'. If None, defaults to '12-31'.\n",
    "    mgrs_tile_id : str, optional\n",
    "        If provided, filters the returned items by the MGRS tile ID.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    items_list : list\n",
    "        A flat list of STAC item objects (granules) that match the search criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    bbox = ([min(lon_range),min(lat_range),max(lon_range),max(lat_range)])\n",
    "    if type(start_year) is int:\n",
    "        years = np.arange(start_year,end_year+1,1)\n",
    "    else:\n",
    "        years = np.arange(int(start_year.split('-')[0]),\n",
    "                          int(end_year.split('-')[0])+1\n",
    "                          ,1)\n",
    "    \n",
    "    # Due to the limitation of the search, split into every year to perform the search\n",
    "    # Note that this might work for a city scale, but for larger domain, the search can\n",
    "    # even further needes to be constrained \n",
    "    items_list = list()\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        collections=['HLSL30.v2.0', 'HLSS30.v2.0']\n",
    "        url='https://cmr.earthdata.nasa.gov/cloudstac/LPCLOUD'\n",
    "        cloudcover_max=50\n",
    "        lim=100\n",
    "        if date_begin is not None:\n",
    "            dt_min = str(year)+'-'+date_begin\n",
    "            dt_max = str(year)+'-'+date_end\n",
    "        else:\n",
    "            dt_min = str(year)+'-01-01'\n",
    "            dt_max = str(year)+'-12-31'\n",
    "            \n",
    "        # open the catalog\n",
    "        catalog = pystac_client.Client.open(f'{url}')\n",
    "        \n",
    "        # perform the search\n",
    "        search = catalog.search(\n",
    "            collections=collections,\n",
    "            bbox=bbox,\n",
    "            datetime=dt_min + '/' + dt_max,\n",
    "            limit=lim\n",
    "        )\n",
    "    \n",
    "        items = list(search.items())\n",
    "\n",
    "        if mgrs_tile_id is not None:\n",
    "            items = [item for item in items if mgrs_tile_id in item.id]\n",
    "                    \n",
    "        \n",
    "        items_list.append(items)\n",
    "        print('Found', len(items), 'granules at point', bbox, 'from', dt_min, 'to', dt_max)\n",
    "\n",
    "    \n",
    "    items_list = [i for item in items_list for i in item]\n",
    "\n",
    "    return items_list\n",
    "\n",
    "\n",
    "def rename_common_bands(items_list):\n",
    "    \"\"\"\n",
    "    Renames the spectral band asset keys in a list of HLS STAC items to a common naming convention.\n",
    "\n",
    "    This function standardizes band names for both Sentinel-2 (HLS.S30) and Landsat (HLS.L30) items by \n",
    "    mapping their band-specific keys to a unified set of descriptive names: \n",
    "    ['Blue', 'Green', 'Red', 'NIR', 'SWIR_1', 'SWIR_2'].\n",
    "\n",
    "    For example:\n",
    "    - HLS.S30 band 'B8A' ‚Üí 'NIR'\n",
    "    - HLS.L30 band 'B05' ‚Üí 'NIR'\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    items_list : list\n",
    "        A list of STAC item objects containing HLS.L30 and/or HLS.S30 assets.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    items_list : list\n",
    "        The same list of STAC items with band asset keys renamed to common names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename HLSS B8A and HLSL B05 to common band name\n",
    "    S30_band_common = ['B02','B03','B04','B8A','B11','B12','B10']\n",
    "    L30_band_common = ['B02','B03','B04','B05','B06','B07','B09']\n",
    "    band_name = ['Blue','Green','Red','NIR','SWIR_1','SWIR_2','Cirrus']\n",
    "    index=0\n",
    "    for band in S30_band_common:\n",
    "        \n",
    "        for item in items_list:\n",
    "            if \"HLS.L30\" in item.id:\n",
    "                item.assets[band_name[index]] = item.assets.pop(L30_band_common[index])\n",
    "            if \"HLS.S30\" in item.id:\n",
    "                item.assets[band_name[index]] = item.assets.pop(band)\n",
    "        \n",
    "        index=index+1\n",
    "            \n",
    "    return items_list\n",
    "\n",
    "def load_odc_stac(crs,bands,spatial_res,items_list,bbox):\n",
    "    \"\"\"\n",
    "    Loads a spatiotemporal datacube from a list of STAC items using the ODC-STAC interface.\n",
    "\n",
    "    This function sets the desired coordinate reference system (CRS), spatial resolution, and bounding box, \n",
    "    and loads the data lazily as an xarray Dataset with Dask chunking.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    crs : str or pyproj.CRS\n",
    "        The target coordinate reference system for the output data (e.g., \"EPSG:32614\").\n",
    "    \n",
    "    bands : list of str\n",
    "        List of band names (e.g., ['Blue', 'Green', 'Red']) to load from the STAC items.\n",
    "\n",
    "    spatial_res : tuple of float\n",
    "        The target spatial resolution as (x_resolution, y_resolution), e.g., (30, 30).\n",
    "\n",
    "    items_list : list\n",
    "        A list of STAC item objects (e.g., from pystac-client or search_cmr_stac) to be loaded.\n",
    "\n",
    "    bbox : list or tuple\n",
    "        Bounding box to constrain the spatial extent of the data, in the form [min_lon, min_lat, max_lon, max_lat].\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds : xarray.Dataset\n",
    "        A lazily-loaded xarray dataset containing the selected bands, spatially and temporally aligned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set CRS and resolution, open lazily with odc.stac\n",
    "    ds = odc.stac.stac_load(\n",
    "        items_list,\n",
    "        bbox=bbox,\n",
    "        bands=(bands),\n",
    "        crs=crs,\n",
    "        resolution=spatial_res,\n",
    "        #chunks=None,\n",
    "        chunks={\"band\":1,\"x\":512,\"y\":512},  # If empty, chunks along band dim, \n",
    "        #groupby=\"solar_day\", # This limits to first obs per day\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "    \n",
    "\n",
    "def get_geometry_clip(city_name):\n",
    "    \"\"\"\n",
    "    Retrieves the bounding box geometry of a specified urban area (city) from a US Census shapefile.\n",
    "\n",
    "    This function loads a shapefile of Urban Areas (2018 vintage from the US Census),\n",
    "    filters it by the given city name, extracts its bounding box, and returns it as a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    city_name : str\n",
    "        The name of the city to extract the bounding box for, matching the 'NAME10' field in the shapefile.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_geo_out : geopandas.GeoDataFrame\n",
    "        A GeoDataFrame containing a single rectangular geometry (bounding box) for the specified city.\n",
    "        The CRS matches that of the original shapefile.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The shapefile must be located at `'cb_2018_us_ua10_500k/cb_2018_us_ua10_500k.shp'` relative to the script.\n",
    "    - The returned geometry is a bounding box, not the full city polygon.\n",
    "    \"\"\"\n",
    "    from shapely.geometry import box\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    df_geo = gpd.read_file('cb_2018_us_ua10_500k/cb_2018_us_ua10_500k.shp')\n",
    "    df_geo_loc = df_geo.loc[df_geo['NAME10'] == city_name]\n",
    "    \n",
    "    # create a bounding box from the shapefile \n",
    "    bounds = df_geo_loc.geometry.bounds.values[0]\n",
    "    geom = box(*bounds)\n",
    "    \n",
    "    df_geo_out = gpd.GeoDataFrame({\"id\":1,\"geometry\":[box(*bounds)]})\n",
    "    df_geo_out = df_geo_out.set_geometry('geometry')\n",
    "    df_geo_out.crs = df_geo.crs\n",
    "\n",
    "    return df_geo_out\n",
    "\n",
    "def scale_hls_data(ds,bands):\n",
    "    \"\"\"\n",
    "    Scales surface reflectance values in selected HLS bands to reflectance units.\n",
    "\n",
    "    HLS data is provided as integer values scaled by a factor of 10,000. This function multiplies \n",
    "    the selected bands (excluding 'Fmask') by 0.0001 to convert them to reflectance values \n",
    "    in the range of approximately 0‚Äì1.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds : xarray.Dataset\n",
    "        The dataset containing HLS bands as data variables.\n",
    "\n",
    "    bands : list of str\n",
    "        List of band names to scale (e.g., ['Blue', 'Green', 'Red', 'NIR']).\n",
    "        Bands containing 'Fmask' in the name will be skipped.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds : xarray.Dataset\n",
    "        The same dataset with the specified bands scaled to reflectance values.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for band in bands:\n",
    "\n",
    "        if 'Fmask' not in band:\n",
    "    \n",
    "            ds[band].data = 0.0001 * ds[band].data\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def configure_gdal_rasterio_dask():\n",
    "    \"\"\"\n",
    "    Configures GDAL, Rasterio, and Xarray to consistently use a custom GDAL environment \n",
    "    for reading remote datasets (e.g., cloud-hosted STAC assets).\n",
    "\n",
    "    This function:\n",
    "    1. Monkey-patches `xarray.Dataset.load()`, `xarray.DataArray.load()`, and \n",
    "       `xarray.Dataset.compute()` to ensure all data loading operations occur within \n",
    "       a custom GDAL environment using `rasterio.Env`.\n",
    "    2. Monkey-patches `rasterio.open()` so any direct file access also respects this \n",
    "       GDAL configuration.\n",
    "\n",
    "    The GDAL environment is customized with the following settings:\n",
    "    - Enables reading cloud-optimized GeoTIFFs (COGs) over HTTP.\n",
    "    - Uses a local cookie file (`~/cookies.txt`) for authenticated access.\n",
    "    - Increases GDAL retry behavior for robustness against intermittent HTTP failures.\n",
    "    - Disables directory reads on open to reduce HTTP overhead.\n",
    "\n",
    "    These patches are helpful in workflows involving:\n",
    "    - NASA's LP DAAC data accessed via STAC endpoints.\n",
    "    - ODCE/Opendatacube-based STAC workflows using `odc.stac.stac_load()`.\n",
    "    - Dask-enabled processing with remote assets and lazy loading.\n",
    "\n",
    "    Dependencies:\n",
    "    ------------\n",
    "    - `xarray`\n",
    "    - `rasterio`\n",
    "    - `rasterio.Env` from `rasterio.env`\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------- Step 1: 1. Monkey‚Äëpatch Xarray‚Äôs .load() to wrap every read in your Env ----------------- \n",
    "    # 1. Grab the real load method *before* patching\n",
    "    _orig_ds_load = xr.Dataset.load\n",
    "    _orig_da_load = xr.DataArray.load\n",
    "    _orig_da_compute = xr.Dataset.compute\n",
    "    \n",
    "    def _load_with_env(self, **kwargs):\n",
    "        # 2. In your Env you can set any GDAL/Rasterio opts\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            # 3. Call the *original* load, not xr.Dataset.load\n",
    "            return _orig_ds_load(self, **kwargs)\n",
    "    \n",
    "    def _da_load_with_env(self, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            return _orig_da_load(self, **kwargs)\n",
    "            \n",
    "    def _da_compute_with_env(self, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            return _orig_da_compute(self, **kwargs)\n",
    "    \n",
    "    # 4. Now monkey‚Äëpatch\n",
    "    xr.Dataset.load   = _load_with_env\n",
    "    xr.DataArray.load = _da_load_with_env\n",
    "    xr.Dataset.compute = _da_compute_with_env\n",
    "    # ----------------- Step 2: Monkey‚Äëpatch rasterio.open itself ----------------- \n",
    "    \n",
    "\n",
    "    # 1. Capture the true open\n",
    "    _orig_open = rasterio.open\n",
    "    \n",
    "    def open_with_env(*args, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            # 2. Call real open\n",
    "            return _orig_open(*args, **kwargs)\n",
    "    \n",
    "    # 3. Replace it\n",
    "    rasterio.open = open_with_env\n",
    "\n",
    "def load_data_into_memory(ds_mask_scaled,split_para=1000):\n",
    "    \n",
    "    \"\"\"\n",
    "    Attempts to load a large xarray.Dataset into memory in chunks to avoid memory or I/O-related failures.\n",
    "\n",
    "    If a full dataset `.load()` fails (e.g., due to size or remote data access limits), this function \n",
    "    falls back to iteratively loading spatial chunks along the x and y dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds_mask_scaled : xarray.Dataset\n",
    "        A dataset with spatial dimensions ('x', 'y') that is lazily loaded (e.g., from a remote source or Dask).\n",
    "    \n",
    "    split_para : int, optional (default=1000)\n",
    "        Defines the spatial chunk size used during fallback loading. Larger values load more data per attempt.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds_mask_scaled_sel : xarray.Dataset\n",
    "        A dataset that has been loaded into memory, either in full or via spatial chunks.\n",
    "\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> ds = odc.stac.stac_load(...)\n",
    "    >>> ds_scaled = scale_hls_data(ds, bands)\n",
    "    >>> ds_loaded = load_data_into_memory(ds_scaled, split_para=512)\n",
    "    \"\"\"\n",
    "    \n",
    "    max_attemp = int(len(ds_mask_scaled.x)/split_para)+2\n",
    "    try:\n",
    "        ds_mask_scaled_sel = ds_mask_scaled\n",
    "        ds_mask_scaled_sel.load() \n",
    "    \n",
    "    except:\n",
    "        \n",
    "        for i in range(max_attemp):\n",
    "    \n",
    "            if i==max_attemp:\n",
    "                ds_mask_scaled_sel = ds_mask_scaled.sel(x=ds_mask_scaled.x[i:],\n",
    "                                                    y=ds_mask_scaled.y[i:])\n",
    "                \n",
    "                ds_mask_scaled_sel = ds_mask_scaled_sel.chunk({'y':split_para*i/5,'x':split_para*i/5})\n",
    "            else:\n",
    "            \n",
    "                ds_mask_scaled_sel = ds_mask_scaled.sel(x=ds_mask_scaled.x[i:split_para*i],\n",
    "                                                    y=ds_mask_scaled.y[i:split_para*i])\n",
    "    \n",
    "                ds_mask_scaled_sel = ds_mask_scaled_sel.chunk({'y':split_para*i/5,'x':split_para*i/5})\n",
    "            \n",
    "            print(i,len(ds_mask_scaled_sel.x),len(ds_mask_scaled_sel.y))\n",
    "            ds_mask_scaled_sel.load()\n",
    "\n",
    "    return ds_mask_scaled_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756b416-390d-440d-b5f1-6596eec39307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_fmask_for_mgrs_tiles(\n",
    "    mrgs_ids_s3,\n",
    "    files_s3,\n",
    "    bucket_name,\n",
    "    temp_dir,\n",
    "    local_dir,\n",
    "    buffer_lat=1.0,\n",
    "    buffer_lon=1.0,\n",
    "    bands=['Blue', 'Green', 'Red', 'Fmask', 'Cirrus']\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper function to download HLS Fmask files from an S3 bucket \n",
    "    for each MGRS tile ID, based on temporal and spatial filters.\n",
    "\n",
    "    Files are exported to:\n",
    "        LOCAL_DIR / SAT_ID / MGRS_ID / filename\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mrgs_ids_s3 : list\n",
    "        List of MGRS tile IDs to process.\n",
    "    files_s3 : list\n",
    "        List of all available S3 file paths in the target bucket.\n",
    "    bucket_name : str\n",
    "        Name of the S3 bucket.\n",
    "    temp_dir : str\n",
    "        Temporary directory for intermediate storage.\n",
    "    local_dir : str\n",
    "        Base local directory for saving final files.\n",
    "    buffer_lat, buffer_lon : float\n",
    "        Spatial buffer (in degrees) around tile center.\n",
    "    bands : list\n",
    "        Bands to download (default includes 'Fmask').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    downloaded_files : list\n",
    "        List of local paths to downloaded files.\n",
    "    \"\"\"\n",
    "    \n",
    "    downloaded_files = []\n",
    "\n",
    "    for mgrs_tile_id in mrgs_ids_s3:\n",
    "        # Find matching S3 files for this tile\n",
    "        files_s3_mgrs = [f for f in files_s3 if mgrs_tile_id in f]\n",
    "        if not files_s3_mgrs:\n",
    "            print(f\"‚ö†Ô∏è No files found for {mgrs_tile_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert file timestamp to Julian day\n",
    "        days_s3 = np.unique([f.split('.')[3] for f in files_s3_mgrs])\n",
    "        \n",
    "        for day_julian in days_s3:\n",
    "            # Determine satellite ID\n",
    "            sat_id = 'S30' if 'S30' in files_s3_mgrs[0].split('/')[2] else 'L30'\n",
    "            \n",
    "            existing_files = glob.glob(os.path.join(local_dir+mgrs_tile_id+'/', f\"HLS.{sat_id}.T{mgrs_tile_id}.{day_julian}*.tif\"))               \n",
    "\n",
    "        \n",
    "        \n",
    "            if len(existing_files) == 5:\n",
    "                print(f\"Skipping {mgrs_tile_id} in date {day_julian}: already processed.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"üîπ Downloading for MGRS ID: {mgrs_tile_id}\")\n",
    "        \n",
    "            # Get tile center\n",
    "            raster_path = f\"s3://{bucket_name}/{files_s3_mgrs[0]}\"\n",
    "            lon, lat = raster_center_latlon(raster_path)\n",
    "        \n",
    "            # Filter files based on the day_julian\n",
    "            files_s3_mgrs_filter = [f for f in files_s3_mgrs if day_julian in f]\n",
    "            \n",
    "\n",
    "            \n",
    "            # Sort files by date\n",
    "            date_str = files_s3_mgrs_filter[0].split('.')[3]  # '2024006'\n",
    "            start_year = date_str[:4]\n",
    "            end_year = date_str[:4] \n",
    "            date_begin = datetime.strptime(date_str, \"%Y%j\").strftime(\"%m-%d\")\n",
    "            date_end = date_begin  # same for single file\n",
    "            \n",
    "            # Define AOI with buffer\n",
    "            lat_range = (lat - buffer_lat, lat + buffer_lat)\n",
    "            lon_range = (lon - buffer_lon, lon + buffer_lon)\n",
    "            \n",
    "            # Search STAC\n",
    "            try:\n",
    "                item_list = search_cmr_stac(start_year, end_year, lat_range, lon_range,\n",
    "                                            date_begin, date_end, mgrs_tile_id=mgrs_tile_id)\n",
    "            except Exception:\n",
    "                item_list = search_cmr_stac(start_year, end_year, lat_range, lon_range,\n",
    "                                            mgrs_tile_id=mgrs_tile_id)\n",
    "            # Filter based on the sat_id\n",
    "            item_list = [i for i in item_list if sat_id in i.id]\n",
    "            \n",
    "        \n",
    "            \n",
    "            if len(item_list) == 0 :\n",
    "                print(f\"‚ö†Ô∏è No matching STAC items found for {mgrs_tile_id} for this date {day_julian}\")\n",
    "                continue\n",
    "            \n",
    "            item_list_rename = rename_common_bands(item_list)\n",
    "            \n",
    "            # Create directory structure: LOCAL_DIR / SAT_ID / MGRS_ID\n",
    "            output_dir = os.path.join(local_dir, mgrs_tile_id)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Download and save files\n",
    "            for item in item_list_rename:\n",
    "                for band in bands:\n",
    "                    try:\n",
    "                        download_url = item.assets[band].href\n",
    "                        local_paths = earthaccess.download(download_url, local_path=output_dir)\n",
    "                        downloaded_files.append(local_paths[0])\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error downloading {band} for {mgrs_tile_id}: {e}\")\n",
    "            \n",
    "            print(f\"‚úÖ Finished {mgrs_tile_id}: saved to {output_dir}\\n\")\n",
    "\n",
    "    return downloaded_files\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
