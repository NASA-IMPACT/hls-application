{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3c0c3-4316-42bb-83ed-bc224e6c0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import odc.stac\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.env import Env\n",
    "import os\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from rasterio.transform import xy\n",
    "from pyproj import Transformer\n",
    "import rioxarray as rxr\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Fmask functions from fmask module\n",
    "%run module/fmask.ipynb\n",
    "\n",
    "def list_all_s3_files_anonymous(bucket_name, prefix=None):\n",
    "    \"\"\"\n",
    "    List all files from a public S3 bucket anonymously (no credentials needed).\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the public S3 bucket.\n",
    "        prefix (str, optional): Filter by a folder or prefix path.\n",
    "\n",
    "    Returns:\n",
    "        list: List of S3 object keys (filenames).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    kwargs = {\"Bucket\": bucket_name}\n",
    "    if prefix:\n",
    "        kwargs[\"Prefix\"] = prefix\n",
    "\n",
    "    files = []\n",
    "    for page in paginator.paginate(**kwargs):\n",
    "        files.extend([obj[\"Key\"] for obj in page.get(\"Contents\", [])])\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "def raster_center_latlon(path):\n",
    "    da = rxr.open_rasterio(path)\n",
    "    h, w = da.rio.height, da.rio.width\n",
    "    x, y = da.rio.transform() * (w/2, h/2)\n",
    "    return Transformer.from_crs(da.rio.crs, \"EPSG:4326\", always_xy=True).transform(x, y)\n",
    "\n",
    "\n",
    "\n",
    "def search_cmr_stac(start_year,end_year,lat_range,lon_range,date_begin=None,date_end=None,mgrs_tile_id=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Searches the CMR STAC API for HLS granules within a specified spatial and temporal range.\n",
    "\n",
    "    This function queries the HLSL30.v2.0 and HLSS30.v2.0 collections from NASA's LPCLOUD STAC endpoint.\n",
    "    The search is split by year to handle API limitations and is most suitable for small to medium spatial domains.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_year : int or str\n",
    "        The beginning year of the search period. Can be an integer (e.g., 2020) or a string (e.g., '2020-01-01').\n",
    "    end_year : int or str\n",
    "        The ending year of the search period (inclusive). Same format as start_year.\n",
    "    lat_range : list or tuple\n",
    "        A pair of latitude values specifying the search bounds, e.g., [min_lat, max_lat].\n",
    "    lon_range : list or tuple\n",
    "        A pair of longitude values specifying the search bounds, e.g., [min_lon, max_lon].\n",
    "    date_begin : str, optional\n",
    "        Optional start date (MM-DD) for each year, e.g., '06-01'. If None, defaults to '01-01'.\n",
    "    date_end : str, optional\n",
    "        Optional end date (MM-DD) for each year, e.g., '09-30'. If None, defaults to '12-31'.\n",
    "    mgrs_tile_id : str, optional\n",
    "        If provided, filters the returned items by the MGRS tile ID.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    items_list : list\n",
    "        A flat list of STAC item objects (granules) that match the search criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    bbox = ([min(lon_range),min(lat_range),max(lon_range),max(lat_range)])\n",
    "    if type(start_year) is int:\n",
    "        years = np.arange(start_year,end_year+1,1)\n",
    "    else:\n",
    "        years = np.arange(int(start_year.split('-')[0]),\n",
    "                          int(end_year.split('-')[0])+1\n",
    "                          ,1)\n",
    "    \n",
    "    # Due to the limitation of the search, split into every year to perform the search\n",
    "    # Note that this might work for a city scale, but for larger domain, the search can\n",
    "    # even further needes to be constrained \n",
    "    items_list = list()\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        collections=['HLSL30.v2.0', 'HLSS30.v2.0']\n",
    "        url='https://cmr.earthdata.nasa.gov/cloudstac/LPCLOUD'\n",
    "        cloudcover_max=50\n",
    "        lim=100\n",
    "        if date_begin is not None:\n",
    "            dt_min = str(year)+'-'+date_begin\n",
    "            dt_max = str(year)+'-'+date_end\n",
    "        else:\n",
    "            dt_min = str(year)+'-01-01'\n",
    "            dt_max = str(year)+'-12-31'\n",
    "            \n",
    "        # open the catalog\n",
    "        catalog = pystac_client.Client.open(f'{url}')\n",
    "        \n",
    "        # perform the search\n",
    "        search = catalog.search(\n",
    "            collections=collections,\n",
    "            bbox=bbox,\n",
    "            datetime=dt_min + '/' + dt_max,\n",
    "            limit=lim\n",
    "        )\n",
    "    \n",
    "        items = list(search.items())\n",
    "\n",
    "        if mgrs_tile_id is not None:\n",
    "            items = [item for item in items if mgrs_tile_id in item.id]\n",
    "                    \n",
    "        \n",
    "        items_list.append(items)\n",
    "        print('Found', len(items), 'granules at point', bbox, 'from', dt_min, 'to', dt_max)\n",
    "\n",
    "    \n",
    "    items_list = [i for item in items_list for i in item]\n",
    "\n",
    "    return items_list\n",
    "\n",
    "\n",
    "def rename_common_bands(items_list):\n",
    "    \"\"\"\n",
    "    Renames the spectral band asset keys in a list of HLS STAC items to a common naming convention.\n",
    "\n",
    "    This function standardizes band names for both Sentinel-2 (HLS.S30) and Landsat (HLS.L30) items by \n",
    "    mapping their band-specific keys to a unified set of descriptive names: \n",
    "    ['Blue', 'Green', 'Red', 'NIR', 'SWIR_1', 'SWIR_2'].\n",
    "\n",
    "    For example:\n",
    "    - HLS.S30 band 'B8A' â†’ 'NIR'\n",
    "    - HLS.L30 band 'B05' â†’ 'NIR'\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    items_list : list\n",
    "        A list of STAC item objects containing HLS.L30 and/or HLS.S30 assets.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    items_list : list\n",
    "        The same list of STAC items with band asset keys renamed to common names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename HLSS B8A and HLSL B05 to common band name\n",
    "    S30_band_common = ['B02','B03','B04','B8A','B11','B12','B10']\n",
    "    L30_band_common = ['B02','B03','B04','B05','B06','B07','B09']\n",
    "    band_name = ['Blue','Green','Red','NIR','SWIR_1','SWIR_2','Cirrus']\n",
    "    index=0\n",
    "    for band in S30_band_common:\n",
    "        \n",
    "        for item in items_list:\n",
    "            if \"HLS.L30\" in item.id:\n",
    "                item.assets[band_name[index]] = item.assets.pop(L30_band_common[index])\n",
    "            if \"HLS.S30\" in item.id:\n",
    "                item.assets[band_name[index]] = item.assets.pop(band)\n",
    "        \n",
    "        index=index+1\n",
    "            \n",
    "    return items_list\n",
    "\n",
    "def load_odc_stac(crs,bands,spatial_res,items_list,bbox):\n",
    "    \"\"\"\n",
    "    Loads a spatiotemporal datacube from a list of STAC items using the ODC-STAC interface.\n",
    "\n",
    "    This function sets the desired coordinate reference system (CRS), spatial resolution, and bounding box, \n",
    "    and loads the data lazily as an xarray Dataset with Dask chunking.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    crs : str or pyproj.CRS\n",
    "        The target coordinate reference system for the output data (e.g., \"EPSG:32614\").\n",
    "    \n",
    "    bands : list of str\n",
    "        List of band names (e.g., ['Blue', 'Green', 'Red']) to load from the STAC items.\n",
    "\n",
    "    spatial_res : tuple of float\n",
    "        The target spatial resolution as (x_resolution, y_resolution), e.g., (30, 30).\n",
    "\n",
    "    items_list : list\n",
    "        A list of STAC item objects (e.g., from pystac-client or search_cmr_stac) to be loaded.\n",
    "\n",
    "    bbox : list or tuple\n",
    "        Bounding box to constrain the spatial extent of the data, in the form [min_lon, min_lat, max_lon, max_lat].\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds : xarray.Dataset\n",
    "        A lazily-loaded xarray dataset containing the selected bands, spatially and temporally aligned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set CRS and resolution, open lazily with odc.stac\n",
    "    ds = odc.stac.stac_load(\n",
    "        items_list,\n",
    "        bbox=bbox,\n",
    "        bands=(bands),\n",
    "        crs=crs,\n",
    "        resolution=spatial_res,\n",
    "        #chunks=None,\n",
    "        chunks={\"band\":1,\"x\":512,\"y\":512},  # If empty, chunks along band dim, \n",
    "        #groupby=\"solar_day\", # This limits to first obs per day\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "    \n",
    "\n",
    "def get_geometry_clip(city_name):\n",
    "    \"\"\"\n",
    "    Retrieves the bounding box geometry of a specified urban area (city) from a US Census shapefile.\n",
    "\n",
    "    This function loads a shapefile of Urban Areas (2018 vintage from the US Census),\n",
    "    filters it by the given city name, extracts its bounding box, and returns it as a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    city_name : str\n",
    "        The name of the city to extract the bounding box for, matching the 'NAME10' field in the shapefile.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_geo_out : geopandas.GeoDataFrame\n",
    "        A GeoDataFrame containing a single rectangular geometry (bounding box) for the specified city.\n",
    "        The CRS matches that of the original shapefile.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The shapefile must be located at `'cb_2018_us_ua10_500k/cb_2018_us_ua10_500k.shp'` relative to the script.\n",
    "    - The returned geometry is a bounding box, not the full city polygon.\n",
    "    \"\"\"\n",
    "    from shapely.geometry import box\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    df_geo = gpd.read_file('cb_2018_us_ua10_500k/cb_2018_us_ua10_500k.shp')\n",
    "    df_geo_loc = df_geo.loc[df_geo['NAME10'] == city_name]\n",
    "    \n",
    "    # create a bounding box from the shapefile \n",
    "    bounds = df_geo_loc.geometry.bounds.values[0]\n",
    "    geom = box(*bounds)\n",
    "    \n",
    "    df_geo_out = gpd.GeoDataFrame({\"id\":1,\"geometry\":[box(*bounds)]})\n",
    "    df_geo_out = df_geo_out.set_geometry('geometry')\n",
    "    df_geo_out.crs = df_geo.crs\n",
    "\n",
    "    return df_geo_out\n",
    "\n",
    "def scale_hls_data(ds,bands):\n",
    "    \"\"\"\n",
    "    Scales surface reflectance values in selected HLS bands to reflectance units.\n",
    "\n",
    "    HLS data is provided as integer values scaled by a factor of 10,000. This function multiplies \n",
    "    the selected bands (excluding 'Fmask') by 0.0001 to convert them to reflectance values \n",
    "    in the range of approximately 0â€“1.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds : xarray.Dataset\n",
    "        The dataset containing HLS bands as data variables.\n",
    "\n",
    "    bands : list of str\n",
    "        List of band names to scale (e.g., ['Blue', 'Green', 'Red', 'NIR']).\n",
    "        Bands containing 'Fmask' in the name will be skipped.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds : xarray.Dataset\n",
    "        The same dataset with the specified bands scaled to reflectance values.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for band in bands:\n",
    "\n",
    "        if 'Fmask' not in band:\n",
    "    \n",
    "            ds[band].data = 0.0001 * ds[band].data\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def configure_gdal_rasterio_dask():\n",
    "    \"\"\"\n",
    "    Configures GDAL, Rasterio, and Xarray to consistently use a custom GDAL environment \n",
    "    for reading remote datasets (e.g., cloud-hosted STAC assets).\n",
    "\n",
    "    This function:\n",
    "    1. Monkey-patches `xarray.Dataset.load()`, `xarray.DataArray.load()`, and \n",
    "       `xarray.Dataset.compute()` to ensure all data loading operations occur within \n",
    "       a custom GDAL environment using `rasterio.Env`.\n",
    "    2. Monkey-patches `rasterio.open()` so any direct file access also respects this \n",
    "       GDAL configuration.\n",
    "\n",
    "    The GDAL environment is customized with the following settings:\n",
    "    - Enables reading cloud-optimized GeoTIFFs (COGs) over HTTP.\n",
    "    - Uses a local cookie file (`~/cookies.txt`) for authenticated access.\n",
    "    - Increases GDAL retry behavior for robustness against intermittent HTTP failures.\n",
    "    - Disables directory reads on open to reduce HTTP overhead.\n",
    "\n",
    "    These patches are helpful in workflows involving:\n",
    "    - NASA's LP DAAC data accessed via STAC endpoints.\n",
    "    - ODCE/Opendatacube-based STAC workflows using `odc.stac.stac_load()`.\n",
    "    - Dask-enabled processing with remote assets and lazy loading.\n",
    "\n",
    "    Dependencies:\n",
    "    ------------\n",
    "    - `xarray`\n",
    "    - `rasterio`\n",
    "    - `rasterio.Env` from `rasterio.env`\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------- Step 1: 1. Monkeyâ€‘patch Xarrayâ€™s .load() to wrap every read in your Env ----------------- \n",
    "    # 1. Grab the real load method *before* patching\n",
    "    _orig_ds_load = xr.Dataset.load\n",
    "    _orig_da_load = xr.DataArray.load\n",
    "    _orig_da_compute = xr.Dataset.compute\n",
    "    \n",
    "    def _load_with_env(self, **kwargs):\n",
    "        # 2. In your Env you can set any GDAL/Rasterio opts\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            # 3. Call the *original* load, not xr.Dataset.load\n",
    "            return _orig_ds_load(self, **kwargs)\n",
    "    \n",
    "    def _da_load_with_env(self, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            return _orig_da_load(self, **kwargs)\n",
    "            \n",
    "    def _da_compute_with_env(self, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            return _orig_da_compute(self, **kwargs)\n",
    "    \n",
    "    # 4. Now monkeyâ€‘patch\n",
    "    xr.Dataset.load   = _load_with_env\n",
    "    xr.DataArray.load = _da_load_with_env\n",
    "    xr.Dataset.compute = _da_compute_with_env\n",
    "    # ----------------- Step 2: Monkeyâ€‘patch rasterio.open itself ----------------- \n",
    "    \n",
    "\n",
    "    # 1. Capture the true open\n",
    "    _orig_open = rasterio.open\n",
    "    \n",
    "    def open_with_env(*args, **kwargs):\n",
    "        with Env(\n",
    "            GDAL_DISABLE_READDIR_ON_OPEN = \"EMPTY_DIR\",\n",
    "            CPL_VSIL_CURL_ALLOWED_EXTENSIONS = \"TIF\",\n",
    "            GDAL_HTTP_COOKIEFILE = '~/cookies.txt',\n",
    "            GDAL_HTTP_COOKIEJAR = '~/cookies.txt',\n",
    "            GDAL_HTTP_UNSAFESSL = 'YES',\n",
    "            GDAL_HTTP_MAX_RETRY = '10',\n",
    "            GDAL_HTTP_RETRY_DELAY = '0.5',\n",
    "            CPL_VSIL_CURL_USE_HEAD = 'YES'\n",
    "        ):\n",
    "            # 2. Call real open\n",
    "            return _orig_open(*args, **kwargs)\n",
    "    \n",
    "    # 3. Replace it\n",
    "    rasterio.open = open_with_env\n",
    "\n",
    "def load_data_into_memory(ds_mask_scaled,split_para=1000):\n",
    "    \n",
    "    \"\"\"\n",
    "    Attempts to load a large xarray.Dataset into memory in chunks to avoid memory or I/O-related failures.\n",
    "\n",
    "    If a full dataset `.load()` fails (e.g., due to size or remote data access limits), this function \n",
    "    falls back to iteratively loading spatial chunks along the x and y dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds_mask_scaled : xarray.Dataset\n",
    "        A dataset with spatial dimensions ('x', 'y') that is lazily loaded (e.g., from a remote source or Dask).\n",
    "    \n",
    "    split_para : int, optional (default=1000)\n",
    "        Defines the spatial chunk size used during fallback loading. Larger values load more data per attempt.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds_mask_scaled_sel : xarray.Dataset\n",
    "        A dataset that has been loaded into memory, either in full or via spatial chunks.\n",
    "\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> ds = odc.stac.stac_load(...)\n",
    "    >>> ds_scaled = scale_hls_data(ds, bands)\n",
    "    >>> ds_loaded = load_data_into_memory(ds_scaled, split_para=512)\n",
    "    \"\"\"\n",
    "    \n",
    "    max_attemp = int(len(ds_mask_scaled.x)/split_para)+2\n",
    "    try:\n",
    "        ds_mask_scaled_sel = ds_mask_scaled\n",
    "        ds_mask_scaled_sel.load() \n",
    "    \n",
    "    except:\n",
    "        \n",
    "        for i in range(max_attemp):\n",
    "    \n",
    "            if i==max_attemp:\n",
    "                ds_mask_scaled_sel = ds_mask_scaled.sel(x=ds_mask_scaled.x[i:],\n",
    "                                                    y=ds_mask_scaled.y[i:])\n",
    "                \n",
    "                ds_mask_scaled_sel = ds_mask_scaled_sel.chunk({'y':split_para*i/5,'x':split_para*i/5})\n",
    "            else:\n",
    "            \n",
    "                ds_mask_scaled_sel = ds_mask_scaled.sel(x=ds_mask_scaled.x[i:split_para*i],\n",
    "                                                    y=ds_mask_scaled.y[i:split_para*i])\n",
    "    \n",
    "                ds_mask_scaled_sel = ds_mask_scaled_sel.chunk({'y':split_para*i/5,'x':split_para*i/5})\n",
    "            \n",
    "            print(i,len(ds_mask_scaled_sel.x),len(ds_mask_scaled_sel.y))\n",
    "            ds_mask_scaled_sel.load()\n",
    "\n",
    "    return ds_mask_scaled_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756b416-390d-440d-b5f1-6596eec39307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_fmask_for_mgrs_tiles(\n",
    "    mrgs_ids_s3,\n",
    "    files_s3,\n",
    "    bucket_name,\n",
    "    temp_dir,\n",
    "    local_dir,\n",
    "    buffer_lat=1.0,\n",
    "    buffer_lon=1.0,\n",
    "    bands=['Blue', 'Green', 'Red', 'Fmask', 'Cirrus']\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper function to download HLS Fmask files from an S3 bucket \n",
    "    for each MGRS tile ID, based on temporal and spatial filters.\n",
    "\n",
    "    Files are exported to:\n",
    "        LOCAL_DIR / SAT_ID / MGRS_ID / filename\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mrgs_ids_s3 : list\n",
    "        List of MGRS tile IDs to process.\n",
    "    files_s3 : list\n",
    "        List of all available S3 file paths in the target bucket.\n",
    "    bucket_name : str\n",
    "        Name of the S3 bucket.\n",
    "    temp_dir : str\n",
    "        Temporary directory for intermediate storage.\n",
    "    local_dir : str\n",
    "        Base local directory for saving final files.\n",
    "    buffer_lat, buffer_lon : float\n",
    "        Spatial buffer (in degrees) around tile center.\n",
    "    bands : list\n",
    "        Bands to download (default includes 'Fmask').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    downloaded_files : list\n",
    "        List of local paths to downloaded files.\n",
    "    \"\"\"\n",
    "    \n",
    "    downloaded_files = []\n",
    "\n",
    "    for mgrs_tile_id in mrgs_ids_s3:\n",
    "        # Find matching S3 files for this tile\n",
    "        files_s3_mgrs = [f for f in files_s3 if mgrs_tile_id in f]\n",
    "        if not files_s3_mgrs:\n",
    "            print(f\"âš ï¸ No files found for {mgrs_tile_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert file timestamp to Julian day\n",
    "        days_s3 = np.unique([f.split('.')[3] for f in files_s3_mgrs])\n",
    "        \n",
    "        for day_julian in days_s3:\n",
    "            # Determine satellite ID\n",
    "            sat_id = 'S30' if 'S30' in files_s3_mgrs[0].split('/')[2] else 'L30'\n",
    "            \n",
    "            existing_files = glob.glob(os.path.join(local_dir+mgrs_tile_id+'/', f\"HLS.{sat_id}.T{mgrs_tile_id}.{day_julian}*.tif\"))               \n",
    "\n",
    "        \n",
    "        \n",
    "            if len(existing_files) == 5:\n",
    "                print(f\"Skipping {mgrs_tile_id} in date {day_julian}: already processed.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ðŸ”¹ Downloading for MGRS ID: {mgrs_tile_id}\")\n",
    "        \n",
    "            # Get tile center\n",
    "            raster_path = f\"s3://{bucket_name}/{files_s3_mgrs[0]}\"\n",
    "            lon, lat = raster_center_latlon(raster_path)\n",
    "        \n",
    "            # Filter files based on the day_julian\n",
    "            files_s3_mgrs_filter = [f for f in files_s3_mgrs if day_julian in f]\n",
    "            \n",
    "\n",
    "            \n",
    "            # Sort files by date\n",
    "            date_str = files_s3_mgrs_filter[0].split('.')[3]  # '2024006'\n",
    "            start_year = date_str[:4]\n",
    "            end_year = date_str[:4] \n",
    "            date_begin = datetime.strptime(date_str, \"%Y%j\").strftime(\"%m-%d\")\n",
    "            date_end = date_begin  # same for single file\n",
    "            \n",
    "            # Define AOI with buffer\n",
    "            lat_range = (lat - buffer_lat, lat + buffer_lat)\n",
    "            lon_range = (lon - buffer_lon, lon + buffer_lon)\n",
    "            \n",
    "            # Search STAC\n",
    "            try:\n",
    "                item_list = search_cmr_stac(start_year, end_year, lat_range, lon_range,\n",
    "                                            date_begin, date_end, mgrs_tile_id=mgrs_tile_id)\n",
    "            except Exception:\n",
    "                item_list = search_cmr_stac(start_year, end_year, lat_range, lon_range,\n",
    "                                            mgrs_tile_id=mgrs_tile_id)\n",
    "            # Filter based on the sat_id\n",
    "            item_list = [i for i in item_list if sat_id in i.id]\n",
    "            \n",
    "        \n",
    "            \n",
    "            if len(item_list) == 0 :\n",
    "                print(f\"âš ï¸ No matching STAC items found for {mgrs_tile_id} for this date {day_julian}\")\n",
    "                continue\n",
    "            \n",
    "            item_list_rename = rename_common_bands(item_list)\n",
    "            \n",
    "            # Create directory structure: LOCAL_DIR / SAT_ID / MGRS_ID\n",
    "            output_dir = os.path.join(local_dir, mgrs_tile_id)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Download and save files\n",
    "            for item in item_list_rename:\n",
    "                for band in bands:\n",
    "                    try:\n",
    "                        download_url = item.assets[band].href\n",
    "                        local_paths = earthaccess.download(download_url, local_path=output_dir)\n",
    "                        downloaded_files.append(local_paths[0])\n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ Error downloading {band} for {mgrs_tile_id}: {e}\")\n",
    "            \n",
    "            print(f\"âœ… Finished {mgrs_tile_id}: saved to {output_dir}\\n\")\n",
    "\n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "def load_fmask_pair(\n",
    "    mrgs_id: str,\n",
    "    bucket: str,\n",
    "    fmask4_files: list[str] | str,\n",
    "    fmask5_files: list[str] | str,\n",
    "):\n",
    "    \"\"\"Load matching Fmask 4.7 and 5.0 images for a given MRGS ID from S3 as time-series DataArrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mrgs_id : str\n",
    "        Granule (MRGS) identifier, e.g., ``'T20LKP'``.\n",
    "    bucket : str\n",
    "        Default S3 bucket name hosting the Fmask data. This is used when\n",
    "        ``fmask4_files`` or ``fmask5_files`` are provided as prefix strings\n",
    "        (not full S3 URIs).\n",
    "    fmask4_files : list[str] | str\n",
    "        Either (1) a pre-computed list of Fmask 4.7 object keys, (2) an S3 URI\n",
    "        (e.g., ``'s3://bucket-name/path/to/folder/'``), or (3) a prefix string\n",
    "        relative to the ``bucket`` parameter.\n",
    "    fmask5_files : list[str] | str\n",
    "        Same options as ``fmask4_files`` but for the Fmask 5.0 assets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with ``'fmask4'`` and ``'fmask5'`` entries containing xarray\n",
    "        DataArrays with dimensions ``('time', 'y', 'x')``. The Fmask values are\n",
    "        decoded to unique classification: 1 (water), 2 (cloud_shadow), 3 (snow_ice),\n",
    "        4 (cloud), NaN (clear/other). Versions without data return ``None``.\n",
    "        Each DataArray includes time coordinates extracted from filenames\n",
    "        (Julian date format: YYYYDDD). Only time steps that exist in both\n",
    "        Fmask 4.7 and 5.0 datasets are returned, ensuring aligned time series.\n",
    "    \"\"\"\n",
    "\n",
    "    def _parse_s3_uri(uri: str) -> tuple[str, str]:\n",
    "        trimmed = uri.strip()\n",
    "        trimmed = trimmed[5:] if trimmed.startswith(\"s3://\") else trimmed\n",
    "        bucket_part, _, prefix_part = trimmed.partition('/')\n",
    "        return bucket_part, prefix_part\n",
    "\n",
    "    def _resolve_source(source, default_bucket):\n",
    "        if isinstance(source, str):\n",
    "            path = source.strip()\n",
    "            if path.startswith(\"s3://\"):\n",
    "                bucket_name, prefix = _parse_s3_uri(path)\n",
    "            else:\n",
    "                bucket_name = default_bucket\n",
    "                prefix = path.lstrip('/')\n",
    "            files = list_all_s3_files_anonymous(bucket_name, prefix=prefix)\n",
    "        else:\n",
    "            # source is a list of file keys\n",
    "            bucket_name = default_bucket\n",
    "            files = source\n",
    "        return bucket_name, files\n",
    "\n",
    "    def _extract_time_from_filename(filename: str) -> str:\n",
    "        \"\"\"Extract Julian date (YYYYDDD) from HLS filename.\n",
    "        \n",
    "        Supports both HLS v2.0 and v5.0 filename formats:\n",
    "        - v2.0: HLS.S30.T20LKP.2024006T105029.v2.0.Fmask.tif\n",
    "        - v5.0: HLS.L30.T31UDQ.2024015.1\n",
    "        \"\"\"\n",
    "        # Try v2.0 format first: 7 digits followed by 'T'\n",
    "        match = re.search(r'\\.(\\d{7})T', filename)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        \n",
    "        # Try v5.0 format: 7 digits followed by '.' or end of string\n",
    "        match = re.search(r'\\.(\\d{7})(?:\\.|$)', filename)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _find_all_keys(file_list, mrgs_id, pattern='Fmask.tif'):\n",
    "        \"\"\"Find all matching keys for the MRGS ID.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_list : list\n",
    "            List of file keys to search\n",
    "        mrgs_id : str\n",
    "            MRGS tile identifier\n",
    "        pattern : str, optional\n",
    "            Pattern to match in filename. Default is 'Fmask.tif'.\n",
    "            Can be 'Fmask.tif', 'Red.tif', 'Green.tif', 'Blue.tif', etc.\n",
    "        \"\"\"\n",
    "        matching_keys = []\n",
    "        for key in file_list:\n",
    "            if key.endswith('.tif') and f'{mrgs_id}.' in key and pattern in key:\n",
    "                matching_keys.append(key)\n",
    "        return sorted(matching_keys)  # Sort for consistent ordering\n",
    "    \n",
    "    def _find_rgb_keys(file_list, mrgs_id):\n",
    "        \"\"\"Find RGB band files (Red, Green, Blue) for the MRGS ID.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with 'Red', 'Green', 'Blue' keys, each containing\n",
    "            a list of matching file keys. Keys with no matches return empty list.\n",
    "        \"\"\"\n",
    "        rgb_bands = {}\n",
    "        # Common band name patterns\n",
    "        band_patterns = {\n",
    "            'Red': ['Red.tif', 'red.tif', 'B04.tif', 'B4.tif', 'Band4.tif'],\n",
    "            'Green': ['Green.tif', 'green.tif', 'B03.tif', 'B3.tif', 'Band3.tif'],\n",
    "            'Blue': ['Blue.tif', 'blue.tif', 'B02.tif', 'B2.tif', 'Band2.tif']\n",
    "        }\n",
    "        \n",
    "        for band_name, patterns in band_patterns.items():\n",
    "            matching = []\n",
    "            for pattern in patterns:\n",
    "                keys = _find_all_keys(file_list, mrgs_id, pattern=pattern)\n",
    "                matching.extend(keys)\n",
    "            rgb_bands[band_name] = sorted(list(set(matching)))  # Remove duplicates and sort\n",
    "        \n",
    "        return rgb_bands\n",
    "    \n",
    "    def _load_rgb_stack(rgb_keys_dict, bucket_name, common_times=None):\n",
    "        \"\"\"Load RGB bands and stack them along time dimension (optimized with parallel loading).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        rgb_keys_dict : dict\n",
    "            Dictionary with 'Red', 'Green', 'Blue' keys containing file lists\n",
    "        bucket_name : str\n",
    "            S3 bucket name\n",
    "        common_times : list, optional\n",
    "            List of datetime objects to filter time steps. If provided, only\n",
    "            loads files matching these times.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict or None\n",
    "            Dictionary with 'Red', 'Green', 'Blue' keys, each containing\n",
    "            an xarray.DataArray with dimensions (time, y, x). Returns None\n",
    "            if no data available. Each band is loaded separately as a\n",
    "            separate DataArray.\n",
    "        \"\"\"\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        \n",
    "        def _load_single_raster(key, bucket_name, common_times):\n",
    "            \"\"\"Load a single raster file and return data with time.\"\"\"\n",
    "            s3_path = f\"/vsis3/{bucket_name}/{key}\"\n",
    "            try:\n",
    "                # Load as DataArray using rioxarray\n",
    "                da = rxr.open_rasterio(s3_path)\n",
    "                # Remove band dimension if present (assuming single band)\n",
    "                if 'band' in da.dims:\n",
    "                    da = da.squeeze('band', drop=True)\n",
    "                \n",
    "                # Extract date string from filename (e.g., '2024366' from '2024366T190105' or '2024358' from '2024358.1')\n",
    "                time_str = _extract_time_from_filename(key)\n",
    "                if time_str:\n",
    "                    # Filter by common_times if provided - simple string comparison\n",
    "                    if common_times is not None:\n",
    "                        # Convert common_times to date strings (YYYYDDD format) for comparison\n",
    "                        def extract_date_string(dt):\n",
    "                            \"\"\"Extract YYYYDDD date string from datetime object - simple conversion.\"\"\"\n",
    "                            import numpy as np\n",
    "                            from datetime import date, datetime\n",
    "                            \n",
    "                            # Get a date object from various datetime types\n",
    "                            if isinstance(dt, str):\n",
    "                                # Already a string - extract first 7 digits if present\n",
    "                                return dt[:7] if len(dt) >= 7 else dt\n",
    "                            \n",
    "                            # Convert to date object first\n",
    "                            try:\n",
    "                                if isinstance(dt, np.datetime64):\n",
    "                                    dt = np.datetime64(dt, 'D').astype(object)\n",
    "                                \n",
    "                                if isinstance(dt, date):\n",
    "                                    d = dt\n",
    "                                elif isinstance(dt, datetime):\n",
    "                                    d = dt.date()\n",
    "                                elif hasattr(dt, 'date'):\n",
    "                                    d = dt.date()\n",
    "                                elif hasattr(dt, 'to_pydatetime'):\n",
    "                                    d = dt.to_pydatetime().date()\n",
    "                                elif hasattr(dt, 'item'):\n",
    "                                    item = dt.item()\n",
    "                                    if isinstance(item, date):\n",
    "                                        d = item\n",
    "                                    elif isinstance(item, datetime):\n",
    "                                        d = item.date()\n",
    "                                    elif isinstance(item, np.datetime64):\n",
    "                                        d = np.datetime64(item, 'D').astype(object)\n",
    "                                        if not isinstance(d, date):\n",
    "                                            d = datetime.strptime(str(d)[:10], '%Y-%m-%d').date()\n",
    "                                    else:\n",
    "                                        # Try to parse as date string\n",
    "                                        d = datetime.strptime(str(item)[:10], '%Y-%m-%d').date()\n",
    "                                else:\n",
    "                                    # Try to parse string representation\n",
    "                                    d = datetime.strptime(str(dt)[:10], '%Y-%m-%d').date()\n",
    "                                \n",
    "                                # Format as YYYYDDD (Julian date)\n",
    "                                return d.strftime('%Y%j')\n",
    "                            except:\n",
    "                                # Fallback: return first 7 characters of string representation\n",
    "                                return str(dt)[:7] if len(str(dt)) >= 7 else str(dt)\n",
    "                        \n",
    "                        # Convert common_times to set of date strings (YYYYDDD format)\n",
    "                        common_date_strings = set([extract_date_string(dt) for dt in common_times])\n",
    "                        \n",
    "                        # Simple string comparison - no datetime conversion needed\n",
    "                        if time_str not in common_date_strings:\n",
    "                            return None, None\n",
    "                    \n",
    "                    # Convert time string to datetime for return value (only when needed)\n",
    "                    try:\n",
    "                        time_dt = datetime.strptime(time_str, '%Y%j')\n",
    "                        return da, time_dt\n",
    "                    except ValueError:\n",
    "                        return da, time_str\n",
    "                else:\n",
    "                    return da, None\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {key}: {e}\")\n",
    "                return None, None\n",
    "        \n",
    "        rgb_data = {}\n",
    "        \n",
    "        for band_name in ['Red', 'Green', 'Blue']:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing {band_name} band...\")\n",
    "            print(f\"{'='*60}\")\n",
    "            keys = rgb_keys_dict.get(band_name, [])\n",
    "            if not keys:\n",
    "                print(f\"  {band_name} band: No files found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Filter keys to only those matching common_times if provided\n",
    "            # (This filters to the pre-selected clear-sky images)\n",
    "            if common_times is not None:\n",
    "                # Convert common_times to date strings for matching\n",
    "                def extract_date_string_simple(dt):\n",
    "                    \"\"\"Extract YYYYDDD date string from datetime object.\"\"\"\n",
    "                    import numpy as np\n",
    "                    from datetime import date, datetime\n",
    "                    if isinstance(dt, str):\n",
    "                        return dt[:7] if len(dt) >= 7 else dt\n",
    "                    try:\n",
    "                        if isinstance(dt, np.datetime64):\n",
    "                            dt = np.datetime64(dt, 'D').astype(object)\n",
    "                        if isinstance(dt, date):\n",
    "                            d = dt\n",
    "                        elif isinstance(dt, datetime):\n",
    "                            d = dt.date()\n",
    "                        elif hasattr(dt, 'date'):\n",
    "                            d = dt.date()\n",
    "                        elif hasattr(dt, 'to_pydatetime'):\n",
    "                            d = dt.to_pydatetime().date()\n",
    "                        elif hasattr(dt, 'item'):\n",
    "                            item = dt.item()\n",
    "                            if isinstance(item, (date, datetime)):\n",
    "                                d = item if isinstance(item, date) else item.date()\n",
    "                            else:\n",
    "                                d = datetime.strptime(str(item)[:10], '%Y-%m-%d').date()\n",
    "                        else:\n",
    "                            d = datetime.strptime(str(dt)[:10], '%Y-%m-%d').date()\n",
    "                        return d.strftime('%Y%j')\n",
    "                    except:\n",
    "                        return str(dt)[:7] if len(str(dt)) >= 7 else str(dt)\n",
    "                \n",
    "                common_date_strings = set([extract_date_string_simple(dt) for dt in common_times])\n",
    "                # Filter keys to only those matching the selected clear-sky times\n",
    "                original_count = len(keys)\n",
    "                filtered_keys = []\n",
    "                for key in keys:\n",
    "                    time_str = _extract_time_from_filename(key)\n",
    "                    if time_str and time_str in common_date_strings:\n",
    "                        filtered_keys.append(key)\n",
    "                keys = filtered_keys\n",
    "                if original_count > len(keys):\n",
    "                    print(f\"  {band_name} band: Filtered to {len(keys)} files matching selected clear-sky times (from {original_count} total)\")\n",
    "            \n",
    "            if not keys:\n",
    "                print(f\"  {band_name} band: No files match the selected time steps, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Parallel loading using ThreadPoolExecutor\n",
    "            dataarrays = []\n",
    "            times = []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "                # Submit all loading tasks\n",
    "                future_to_key = {\n",
    "                    executor.submit(_load_single_raster, key, bucket_name, common_times): key\n",
    "                    for key in keys\n",
    "                }\n",
    "                \n",
    "                # Collect results as they complete\n",
    "                total_files = len(keys)\n",
    "                loaded_count = 0\n",
    "                skipped_count = 0\n",
    "                processed_count = 0\n",
    "                \n",
    "                print(f\"Loading {band_name} band: {total_files} files total\")\n",
    "                \n",
    "                for future in as_completed(future_to_key):\n",
    "                    da, time_val = future.result()\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                    if da is not None:\n",
    "                        dataarrays.append(da)\n",
    "                        if time_val is not None:\n",
    "                            times.append(time_val)\n",
    "                        else:\n",
    "                            times.append(len(dataarrays) - 1)\n",
    "                        loaded_count += 1\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                    \n",
    "                    # Progress tracking: show percentage and remaining\n",
    "                    percentage = (processed_count / total_files) * 100\n",
    "                    remaining = total_files - processed_count\n",
    "                    print(f\"  {band_name} band progress: {processed_count}/{total_files} ({percentage:.1f}%) - \"\n",
    "                          f\"Loaded: {loaded_count}, Skipped: {skipped_count}, Remaining: {remaining}\")\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "            if dataarrays:\n",
    "                # Memory monitoring before concatenation\n",
    "                try:\n",
    "                    import psutil\n",
    "                    import os\n",
    "                    process = psutil.Process(os.getpid())\n",
    "                    mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "                    print(f\"  {band_name} band: Starting concatenation of {len(dataarrays)} arrays...\")\n",
    "                    print(f\"  {band_name} band: Memory before concatenation: {mem_before:.1f} MB\")\n",
    "                except ImportError:\n",
    "                    print(f\"  {band_name} band: Starting concatenation of {len(dataarrays)} arrays...\")\n",
    "                    print(f\"  {band_name} band: (psutil not available for memory monitoring)\")\n",
    "                \n",
    "                # Concatenate along time dimension using chunked approach for large datasets\n",
    "                try:\n",
    "                    print(f\"  {band_name} band: Concatenating {len(dataarrays)} arrays...\")\n",
    "                    \n",
    "                    # Use chunked concatenation for large datasets to avoid memory issues\n",
    "                    chunk_size = 10  # Concatenate 10 arrays at a time\n",
    "                    if len(dataarrays) > chunk_size:\n",
    "                        print(f\"  {band_name} band: Using chunked concatenation (chunk size: {chunk_size})\")\n",
    "                        chunks = []\n",
    "                        time_chunks = []\n",
    "                        \n",
    "                        # Process in chunks\n",
    "                        for i in range(0, len(dataarrays), chunk_size):\n",
    "                            chunk_end = min(i + chunk_size, len(dataarrays))\n",
    "                            chunk_arrays = dataarrays[i:chunk_end]\n",
    "                            chunk_times = times[i:chunk_end]\n",
    "                            \n",
    "                            print(f\"  {band_name} band: Concatenating chunk {i//chunk_size + 1}/{(len(dataarrays)-1)//chunk_size + 1} \"\n",
    "                                  f\"(arrays {i+1}-{chunk_end})...\")\n",
    "                            \n",
    "                            # Concatenate this chunk\n",
    "                            chunk_concat = xr.concat(chunk_arrays, dim='time')\n",
    "                            chunk_concat = chunk_concat.assign_coords(time=chunk_times)\n",
    "                            chunks.append(chunk_concat)\n",
    "                            time_chunks.extend(chunk_times)\n",
    "                            \n",
    "                            # Clear chunk from memory\n",
    "                            del chunk_arrays, chunk_concat\n",
    "                        \n",
    "                        # Concatenate all chunks together\n",
    "                        print(f\"  {band_name} band: Concatenating {len(chunks)} chunks into final array...\")\n",
    "                        stacked = xr.concat(chunks, dim='time')\n",
    "                        stacked = stacked.assign_coords(time=time_chunks)\n",
    "                        \n",
    "                        # Clear chunks from memory\n",
    "                        del chunks\n",
    "                    else:\n",
    "                        # For small datasets, concatenate directly\n",
    "                        stacked = xr.concat(dataarrays, dim='time')\n",
    "                        stacked = stacked.assign_coords(time=times)\n",
    "                    \n",
    "                    print(f\"  {band_name} band: Concatenation complete, assigning coordinates...\")\n",
    "                    print(f\"  {band_name} band: Storing {band_name} band data...\")\n",
    "                    rgb_data[band_name] = stacked\n",
    "                    \n",
    "                    # Clear dataarrays from memory to free up space\n",
    "                    del dataarrays\n",
    "                    dataarrays = []  # Reset for next band if needed\n",
    "                    \n",
    "                    # Show data size information\n",
    "                    try:\n",
    "                        data_size_mb = stacked.nbytes / 1024 / 1024\n",
    "                        print(f\"  {band_name} band: Data size: {data_size_mb:.1f} MB, Shape: {stacked.shape}\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Memory monitoring after concatenation\n",
    "                    try:\n",
    "                        mem_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "                        mem_used = mem_after - mem_before\n",
    "                        print(f\"  {band_name} band: Memory after concatenation: {mem_after:.1f} MB (used: {mem_used:.1f} MB)\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    print(f\"  âœ“ {band_name} band complete: {loaded_count} files loaded, {skipped_count} skipped\")\n",
    "                except MemoryError as e:\n",
    "                    print(f\"  âœ— {band_name} band: Memory error during concatenation: {e}\")\n",
    "                    print(f\"  {band_name} band: Consider loading fewer files or using chunked processing\")\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    print(f\"  âœ— {band_name} band: Error during concatenation: {e}\")\n",
    "                    print(f\"  {band_name} band: Error type: {type(e).__name__}\")\n",
    "                    raise\n",
    "            else:\n",
    "                print(f\"  âœ— {band_name} band: No files loaded (all {skipped_count} files were skipped)\")\n",
    "        \n",
    "        if not rgb_data:\n",
    "            print(\"\\nNo RGB data loaded - returning None\")\n",
    "            return None\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"RGB loading complete!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        loaded_bands = list(rgb_data.keys())\n",
    "        print(f\"Successfully loaded bands: {loaded_bands}\")\n",
    "        for band_name in loaded_bands:\n",
    "            try:\n",
    "                band_size_mb = rgb_data[band_name].nbytes / 1024 / 1024\n",
    "                print(f\"  {band_name}: {rgb_data[band_name].shape}, {band_size_mb:.1f} MB\")\n",
    "            except:\n",
    "                print(f\"  {band_name}: {rgb_data[band_name].shape}\")\n",
    "        \n",
    "        try:\n",
    "            import psutil\n",
    "            import os\n",
    "            process = psutil.Process(os.getpid())\n",
    "            total_mem = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            print(f\"\\nTotal memory usage: {total_mem:.1f} MB\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Return dictionary of separate DataArrays for each band\n",
    "        return rgb_data\n",
    "\n",
    "    def _load_raster_stack(keys, bucket_name):\n",
    "        \"\"\"Load multiple rasters, decode to unique bits, and stack them along time dimension.\"\"\"\n",
    "        if not keys:\n",
    "            return None\n",
    "        \n",
    "        dataarrays = []\n",
    "        times = []\n",
    "        \n",
    "        for key in keys:\n",
    "            s3_path = f\"/vsis3/{bucket_name}/{key}\"\n",
    "            try:\n",
    "                # Load as DataArray using rioxarray\n",
    "                da = rxr.open_rasterio(s3_path)\n",
    "                # Remove band dimension if present (assuming single band)\n",
    "                if 'band' in da.dims:\n",
    "                    da = da.squeeze('band', drop=True)\n",
    "                \n",
    "                # Decode Fmask to unique classification values\n",
    "                decoded = convert_fmask_unique(da)\n",
    "                \n",
    "                # Convert back to DataArray preserving coordinates and attributes\n",
    "                da_decoded = xr.DataArray(\n",
    "                    decoded,\n",
    "                    dims=da.dims,\n",
    "                    coords=da.coords,\n",
    "                    attrs=da.attrs\n",
    "                )\n",
    "                \n",
    "                # Extract time from filename\n",
    "                time_str = _extract_time_from_filename(key)\n",
    "                if time_str:\n",
    "                    # Convert Julian date to datetime\n",
    "                    try:\n",
    "                        time_dt = datetime.strptime(time_str, '%Y%j')\n",
    "                        times.append(time_dt)\n",
    "                    except ValueError:\n",
    "                        # Fallback: use the string as-is\n",
    "                        times.append(time_str)\n",
    "                else:\n",
    "                    # Use index if time extraction fails\n",
    "                    times.append(len(dataarrays))\n",
    "                \n",
    "                dataarrays.append(da_decoded)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not dataarrays:\n",
    "            return None\n",
    "        \n",
    "        # Concatenate along time dimension\n",
    "        stacked = xr.concat(dataarrays, dim='time')\n",
    "        stacked = stacked.assign_coords(time=times)\n",
    "        \n",
    "        return stacked\n",
    "\n",
    "    bucket4, fmask4_files = _resolve_source(fmask4_files, bucket)\n",
    "    bucket5, fmask5_files = _resolve_source(fmask5_files, bucket)\n",
    "\n",
    "    keys4 = _find_all_keys(fmask4_files, mrgs_id)\n",
    "    keys5 = _find_all_keys(fmask5_files, mrgs_id)\n",
    "    \n",
    "    # Diagnostic output\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading Fmask data for MRGS ID: {mrgs_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Fmask 4.7: Found {len(keys4)} matching files in bucket '{bucket4}'\")\n",
    "    if len(keys4) > 0:\n",
    "        print(f\"  Sample files (first 3):\")\n",
    "        for key in keys4[:3]:\n",
    "            print(f\"    - {key}\")\n",
    "    else:\n",
    "        print(f\"  Searching in {len(fmask4_files)} total files\")\n",
    "        # Show sample of available files to help debug\n",
    "        sample_files = [f for f in fmask4_files if mrgs_id in f][:5]\n",
    "        if sample_files:\n",
    "            print(f\"  Sample files containing '{mrgs_id}' (first 5):\")\n",
    "            for f in sample_files:\n",
    "                print(f\"    - {f}\")\n",
    "        else:\n",
    "            print(f\"  No files found containing '{mrgs_id}'\")\n",
    "    \n",
    "    print(f\"Fmask 5.0: Found {len(keys5)} matching files in bucket '{bucket5}'\")\n",
    "    if len(keys5) > 0:\n",
    "        print(f\"  Sample files (first 3):\")\n",
    "        for key in keys5[:3]:\n",
    "            print(f\"    - {key}\")\n",
    "    else:\n",
    "        print(f\"  Searching in {len(fmask5_files)} total files\")\n",
    "        # Show sample of available files to help debug\n",
    "        sample_files = [f for f in fmask5_files if mrgs_id in f][:5]\n",
    "        if sample_files:\n",
    "            print(f\"  Sample files containing '{mrgs_id}' (first 5):\")\n",
    "            for f in sample_files:\n",
    "                print(f\"    - {f}\")\n",
    "        else:\n",
    "            print(f\"  No files found containing '{mrgs_id}'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Load both datasets\n",
    "    fmask4_stack = _load_raster_stack(keys4, bucket4)\n",
    "    fmask5_stack = _load_raster_stack(keys5, bucket5)\n",
    "    \n",
    "    # Match time steps: only return common time steps\n",
    "    if fmask4_stack is not None and fmask5_stack is not None:\n",
    "        # Find common time values\n",
    "        time4 = set(fmask4_stack.time.values)\n",
    "        time5 = set(fmask5_stack.time.values)\n",
    "        common_times = sorted(list(time4.intersection(time5)))\n",
    "        \n",
    "        if len(common_times) == 0:\n",
    "            print(\"Warning: No common time steps found between Fmask 4.7 and 5.0\")\n",
    "            return {\n",
    "                'fmask4': None,\n",
    "                'fmask5': None,\n",
    "                'fmask4_red': None,\n",
    "                'fmask4_green': None,\n",
    "                'fmask4_blue': None,\n",
    "            }\n",
    "        \n",
    "        # Select only common time steps\n",
    "        fmask4_matched = fmask4_stack.sel(time=common_times)\n",
    "        fmask5_matched = fmask5_stack.sel(time=common_times)\n",
    "        \n",
    "        print(f\"Matched {len(common_times)} common time steps out of \"\n",
    "              f\"{len(time4)} (Fmask 4.7) and {len(time5)} (Fmask 5.0) total time steps\")\n",
    "        \n",
    "        # Helper function to select top N clear-sky images based on cloud coverage\n",
    "        def select_clear_sky_images(fmask_data, n=10):\n",
    "            \"\"\"\n",
    "            Select top N clear-sky images (lowest cloud coverage) from Fmask data.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            fmask_data : xarray.DataArray\n",
    "                Fmask DataArray with dimensions (time, y, x)\n",
    "                Values: 1=water, 2=cloud_shadow, 3=snow_ice, 4=cloud, NaN=clear\n",
    "            n : int\n",
    "                Number of clear-sky images to select (default: 10)\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            list\n",
    "                List of time values (datetime objects) for the top N clear-sky images\n",
    "            \"\"\"\n",
    "            if fmask_data is None or 'time' not in fmask_data.dims:\n",
    "                return []\n",
    "            \n",
    "            import numpy as np\n",
    "            \n",
    "            # Calculate cloud coverage percentage for each time step\n",
    "            # Cloud pixels are where value == 4\n",
    "            cloud_coverage = []\n",
    "            time_values = []\n",
    "            \n",
    "            for time_val in fmask_data.time.values:\n",
    "                time_slice = fmask_data.sel(time=time_val)\n",
    "                \n",
    "                # Count cloud pixels (value == 4)\n",
    "                cloud_pixels = (time_slice == 4).sum().values\n",
    "                \n",
    "                # Count total valid (non-NaN) pixels\n",
    "                valid_pixels = (~np.isnan(time_slice)).sum().values\n",
    "                \n",
    "                if valid_pixels > 0:\n",
    "                    # Calculate cloud coverage percentage\n",
    "                    coverage_pct = (cloud_pixels / valid_pixels) * 100.0\n",
    "                    cloud_coverage.append(coverage_pct)\n",
    "                    time_values.append(time_val)\n",
    "                else:\n",
    "                    # Skip time steps with no valid data\n",
    "                    continue\n",
    "            \n",
    "            if not cloud_coverage:\n",
    "                return []\n",
    "            \n",
    "            # Sort by cloud coverage (ascending - least clouds first)\n",
    "            # Get indices sorted by cloud coverage\n",
    "            sorted_indices = np.argsort(cloud_coverage)\n",
    "            \n",
    "            # Select top N clear-sky images\n",
    "            n_selected = min(n, len(sorted_indices))\n",
    "            selected_indices = sorted_indices[:n_selected]\n",
    "            selected_times = [time_values[i] for i in selected_indices]\n",
    "            selected_coverage = [cloud_coverage[i] for i in selected_indices]\n",
    "            \n",
    "            print(f\"Selected top {n_selected} clear-sky images:\")\n",
    "            print(f\"  Cloud coverage range: {min(selected_coverage):.2f}% - {max(selected_coverage):.2f}%\")\n",
    "            print(f\"  Average cloud coverage: {np.mean(selected_coverage):.2f}%\")\n",
    "            \n",
    "            return selected_times\n",
    "        \n",
    "        # Select top 10 clear-sky images from Fmask 4.7 for RGB loading\n",
    "        clear_sky_times = select_clear_sky_images(fmask4_matched, n=10)\n",
    "        \n",
    "        # Load RGB bands for Fmask 4.7 if available (separate bands)\n",
    "        # Only load RGB for the selected clear-sky images\n",
    "        fmask4_rgb_dict = None\n",
    "        try:\n",
    "            rgb_keys = _find_rgb_keys(fmask4_files, mrgs_id)\n",
    "            if any(rgb_keys.values()):  # Check if any RGB bands found\n",
    "                if clear_sky_times:\n",
    "                    print(f\"Loading RGB bands for {len(clear_sky_times)} clear-sky images...\")\n",
    "                    fmask4_rgb_dict = _load_rgb_stack(rgb_keys, bucket4, common_times=clear_sky_times)\n",
    "                else:\n",
    "                    print(\"Warning: No clear-sky images found, skipping RGB loading\")\n",
    "                if fmask4_rgb_dict is not None:\n",
    "                    loaded_bands = [k for k in ['Red', 'Green', 'Blue'] if k in fmask4_rgb_dict]\n",
    "                    print(f\"Loaded RGB bands for Fmask 4.7: {loaded_bands}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load RGB bands for Fmask 4.7: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'fmask4': fmask4_matched,\n",
    "            'fmask5': fmask5_matched,\n",
    "            'fmask4_red': fmask4_rgb_dict.get('Red') if fmask4_rgb_dict else None,\n",
    "            'fmask4_green': fmask4_rgb_dict.get('Green') if fmask4_rgb_dict else None,\n",
    "            'fmask4_blue': fmask4_rgb_dict.get('Blue') if fmask4_rgb_dict else None,\n",
    "        }\n",
    "\n",
    "    elif fmask4_stack is not None:\n",
    "        print(\"Warning: Fmask 5.0 data not available\")\n",
    "        \n",
    "        # Helper function to select top N clear-sky images based on cloud coverage\n",
    "        def select_clear_sky_images_fmask4_only(fmask_data, n=10):\n",
    "            \"\"\"\n",
    "            Select top N clear-sky images (lowest cloud coverage) from Fmask data.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            fmask_data : xarray.DataArray\n",
    "                Fmask DataArray with dimensions (time, y, x)\n",
    "                Values: 1=water, 2=cloud_shadow, 3=snow_ice, 4=cloud, NaN=clear\n",
    "            n : int\n",
    "                Number of clear-sky images to select (default: 10)\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            list\n",
    "                List of time values (datetime objects) for the top N clear-sky images\n",
    "            \"\"\"\n",
    "            if fmask_data is None or 'time' not in fmask_data.dims:\n",
    "                return []\n",
    "            \n",
    "            import numpy as np\n",
    "            \n",
    "            # Calculate cloud coverage percentage for each time step\n",
    "            cloud_coverage = []\n",
    "            time_values = []\n",
    "            \n",
    "            for time_val in fmask_data.time.values:\n",
    "                time_slice = fmask_data.sel(time=time_val)\n",
    "                cloud_pixels = (time_slice == 4).sum().values\n",
    "                valid_pixels = (~np.isnan(time_slice)).sum().values\n",
    "                \n",
    "                if valid_pixels > 0:\n",
    "                    coverage_pct = (cloud_pixels / valid_pixels) * 100.0\n",
    "                    cloud_coverage.append(coverage_pct)\n",
    "                    time_values.append(time_val)\n",
    "            \n",
    "            if not cloud_coverage:\n",
    "                return []\n",
    "            \n",
    "            sorted_indices = np.argsort(cloud_coverage)\n",
    "            n_selected = min(n, len(sorted_indices))\n",
    "            selected_indices = sorted_indices[:n_selected]\n",
    "            selected_times = [time_values[i] for i in selected_indices]\n",
    "            selected_coverage = [cloud_coverage[i] for i in selected_indices]\n",
    "            \n",
    "            print(f\"Selected top {n_selected} clear-sky images:\")\n",
    "            print(f\"  Cloud coverage range: {min(selected_coverage):.2f}% - {max(selected_coverage):.2f}%\")\n",
    "            print(f\"  Average cloud coverage: {np.mean(selected_coverage):.2f}%\")\n",
    "            \n",
    "            return selected_times\n",
    "        \n",
    "        # Select top 10 clear-sky images from Fmask 4.7 for RGB loading\n",
    "        clear_sky_times = select_clear_sky_images_fmask4_only(fmask4_stack, n=10)\n",
    "        \n",
    "        # Try to load RGB bands (separate bands)\n",
    "        # Only load RGB for the selected clear-sky images\n",
    "        fmask4_rgb_dict = None\n",
    "        try:\n",
    "            rgb_keys = _find_rgb_keys(fmask4_files, mrgs_id)\n",
    "            if any(rgb_keys.values()):\n",
    "                if clear_sky_times:\n",
    "                    print(f\"Loading RGB bands for {len(clear_sky_times)} clear-sky images...\")\n",
    "                    fmask4_rgb_dict = _load_rgb_stack(rgb_keys, bucket4, common_times=clear_sky_times)\n",
    "                else:\n",
    "                    print(\"Warning: No clear-sky images found, skipping RGB loading\")\n",
    "                if fmask4_rgb_dict is not None:\n",
    "                    loaded_bands = [k for k in ['Red', 'Green', 'Blue'] if k in fmask4_rgb_dict]\n",
    "                    print(f\"Loaded RGB bands for Fmask 4.7: {loaded_bands}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load RGB bands for Fmask 4.7: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'fmask4': fmask4_stack,\n",
    "            'fmask5': None,\n",
    "            'fmask4_red': fmask4_rgb_dict.get('Red') if fmask4_rgb_dict else None,\n",
    "            'fmask4_green': fmask4_rgb_dict.get('Green') if fmask4_rgb_dict else None,\n",
    "            'fmask4_blue': fmask4_rgb_dict.get('Blue') if fmask4_rgb_dict else None,\n",
    "        }\n",
    "        \n",
    "    elif fmask5_stack is not None:\n",
    "        print(\"Warning: Fmask 4.7 data not available\")\n",
    "        return {\n",
    "            'fmask4': None,\n",
    "            'fmask5': fmask5_stack,\n",
    "            'fmask4_red': None,\n",
    "            'fmask4_green': None,\n",
    "            'fmask4_blue': None,\n",
    "        }\n",
    "    else:\n",
    "        print(\"Warning: No Fmask data available\")\n",
    "        return {\n",
    "            'fmask4': None,\n",
    "            'fmask5': None,\n",
    "            'fmask4_red': None,\n",
    "            'fmask4_green': None,\n",
    "            'fmask4_blue': None,\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
