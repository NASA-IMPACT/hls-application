{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be07a7a3-7d1a-4b9d-a201-528ad5a4a392",
   "metadata": {},
   "source": [
    "## This notebook has functions to \n",
    "- open a .tif file as a raster dataset\n",
    "- read the raster dataset and map spectral bands to a common bands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fab3246-2bd4-4184-89b6-288dcf6da44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(base, band, band_name, cmr=False, coords=None, start_date=None, end_date=None):\n",
    "    '''\n",
    "    Function to read each file from S3 bucket url or fetch from CMR. If fetch from CMR, download fiiles to current working \n",
    "    directory (e.g., grive and read from there)\n",
    "    '''\n",
    "    \n",
    "    ## Open raster and apply scale and offset\n",
    "    print(f\"Reading {base}.{band}.tif\")\n",
    "    da = rxr.open_rasterio(f\"{base}.{band}.tif\", mask_and_scale=True)\n",
    "    da.name = band_name\n",
    "    \n",
    "    return da \n",
    "\n",
    "\n",
    "def read_file_as_array(sat_id,run_id=None,title_id=None,granule=None,sr_key=None):\n",
    "    '''\n",
    "    Function to read all bands included in the dataset \n",
    "    \n",
    "    Example:\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Map Spectral Bands\n",
    "    # Ignore Coastal aerosol and Cirrus bands\n",
    "    if 'L30' in sat_id:\n",
    "        sr_bands = [\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\", \"Fmask\"]\n",
    "    else:\n",
    "        sr_bands = [\"B02\",\"B03\",\"B04\",\"B8A\",\"B11\",\"B12\", \"Fmask\"]\n",
    "        \n",
    "\n",
    "    common_bands = [\"B\",\"G\",\"R\",\"NIR\",\"SWIR1\", \"SWIR2\", 'Fmask']\n",
    "    \n",
    "    sr_bands_common = dict(zip(common_bands,sr_bands))\n",
    "\n",
    "    \n",
    "    print(\"Reading \"+sat_id+\" raster data\")\n",
    "    \n",
    "    sr_das = [open_file(sr_key, \n",
    "                            band, \n",
    "                            band_name) for (band,\n",
    "                                            band_name) in zip(sr_bands, \n",
    "                                                              common_bands)]\n",
    "                                                              \n",
    "    sr_ds  = xr.merge(sr_das, \n",
    "                          combine_attrs=\"drop_conflicts\") \n",
    "    \n",
    "    \n",
    "    return sr_ds\n",
    "\n",
    "    \n",
    "def search_stac_for_HLS(pt, dt_min, dt_max, cloudcover_max=50, lim=100, url='https://cmr.earthdata.nasa.gov/cloudstac/LPCLOUD', \n",
    "                        collections=['HLSL30.v2.0', 'HLSS30.v2.0']):\n",
    "    # open the catalog\n",
    "    catalog = Client.open(f'{url}')\n",
    "    \n",
    "    # perform the search\n",
    "    search = catalog.search(\n",
    "        collections=collections,\n",
    "        intersects=pt,\n",
    "        datetime=dt_min + '/' + dt_max,\n",
    "        limit=lim\n",
    "    )\n",
    "\n",
    "    links = []\n",
    "\n",
    "    if search.matched() == 0:\n",
    "        print('No granules found at point', pt, 'from', dt_min, 'to', dt_max)\n",
    "    else:\n",
    "        print('Found', search.matched(), 'granules at point', pt, 'from', dt_min, 'to', dt_max)\n",
    "        item_collection = search.get_all_items()\n",
    "        \n",
    "        for i in item_collection:\n",
    "            for a in i.assets:\n",
    "                    links.append(i.assets[a].href)\n",
    "\n",
    "    return(links)\n",
    "\n",
    "\n",
    "def fix_links(src_link, src_dirs, dst_dir, meta_dir, add_tile_dir=True):\n",
    "    dst_link = src_link\n",
    "\n",
    "    if '.xml' in dst_link:\n",
    "        dst_link2 = os.path.join(meta_dir, os.path.basename(dst_link))\n",
    "    else:\n",
    "        for src_dir in src_dirs:\n",
    "            dst_link = dst_link.replace(src_dir, dst_dir)\n",
    "        dst_splits = dst_link.split('/')\n",
    "        dst_link2 = '/'.join(dst_splits[0:2]) + \\\n",
    "            '/' + dst_splits[3].split('.')[2] + \\\n",
    "            '/' + '/'.join(dst_splits[3:])\n",
    "    \n",
    "    return(dst_link2)\n",
    "\n",
    "def get_temp_creds(url,user,password):\n",
    "    #url = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n",
    "    url = requests.get(url, allow_redirects=False).headers['Location']\n",
    "    creds = requests.get(url, auth=(user, password)).json()\n",
    "    return creds\n",
    "\n",
    "\n",
    "\n",
    "def make_dirs(dst_links):\n",
    "    try:\n",
    "        for dst_link in dst_links:\n",
    "            os.makedirs(os.path.dirname(dst_link), exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def download_data(s3_links, local_links, s3_session):\n",
    "    s3_links = [l.replace('s3://', '') for l in s3_links]\n",
    "    for i in range(0, len(s3_links)):\n",
    "        s3_link = s3_links[i]\n",
    "        s3_bucket = s3_link.split('/')[0]\n",
    "        s3_link = s3_link.replace(s3_bucket +'/', '')   \n",
    "        #print(s3_link)\n",
    "        local_link = local_links[i]\n",
    "        prefix = ''\n",
    "        delimiter = '/'\n",
    "        \n",
    "        # ignore XML files for now, figure out how to get them later because they contain useful information\n",
    "        if not '.xml' in local_link:\n",
    "            if not '.jpg' in local_link:\n",
    "                try:\n",
    "                    with open(local_link, 'wb') as f:\n",
    "                        print(i, s3_bucket, s3_link, local_link)\n",
    "                        #s3.download_fileobj(s3_bucket, s3_link, f)\n",
    "                        bucket = 'lp-prod-protected'\n",
    "                        prefix = ''\n",
    "                        delimiter = '/'\n",
    "                        s3_session.download_file(Bucket=bucket, \n",
    "                                         Key=s3_link, \n",
    "                                         Filename=local_link)\n",
    "\n",
    "                  \n",
    "        \n",
    "\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    #print('Errors with file'+s3_link)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69932e57-1d38-4407-8716-fe3ff7e12055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30eae4fb-c957-4d23-b79c-6dab42ebb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dd2e6c1f-cf0e-4ec0-bd82-480e68934d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c12ec25-edaf-4983-bc04-f6814783d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1287ca0b-dfe9-47c7-9298-3277ef6d65a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8741cc76-98d8-468b-97e7-abecaa33c567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75771f7a-b46d-4109-928d-84e5d9d1ca4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
